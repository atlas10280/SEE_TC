{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "########## Dependencies\n",
    "##### define neural net weights\n",
    "autoencoder_path = \"/mnt/local/data2/Bootsma/2D_CTC/src/analysis/publication_code/weights/02_SEE_TC_encoder.hdf5\"\n",
    "classifier_path = \"/mnt/dho-nas06/zhaolab/long_term_storage/pipeline/digital_pathology/cellClass_2Channel_GBoost.hdf5\"\n",
    "##### define samples to process\n",
    "img_dir_qa = \"/mnt/local/data2/Bootsma/2D_CTC/src/analysis/publication_code/test_data/\"\n",
    "\n",
    "##### define GPU indices to use\n",
    "gpu_id = \"0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# define the channels for exclusion and pCK, just the first integer will be needed\n",
    "pCK_name = \"7\"\n",
    "exclusion_name = \"6\"\n",
    "\n",
    "# ##### define suffix for input tiff\n",
    "# input_tiff_suffix = \".bias_corrected.tiff\"\n",
    "\n",
    "# ##### define suffix for output table \n",
    "# # must match the suffix of your physical feature table\n",
    "# # results will be appended into said table\n",
    "# feature_suffix = \".region_features.tsv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Dependencies\n",
    "import sys\n",
    "sys.path.append('../src/') \n",
    "import SEE_TC as ctc\n",
    "# import preprocessing.CTC_2d_preprocessing as ctc_pp\n",
    "import umap\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "def count_labels(y_pred):\n",
    "    unique_vals, counts = np.unique(y_pred, return_counts=True)\n",
    "    freq_dict = dict(zip(unique_vals, counts))\n",
    "    print(freq_dict)\n",
    "\n",
    "##### NORMALIZE DATA - method A; min-max scale across channels to maintain relative intensity\n",
    "def normalize_CNN_input_ch6(z_stack_in):\n",
    "        \n",
    "    z_stack_c = z_stack_in.copy()\n",
    "    z_stack_c = z_stack_c[:,:,:,[0,1,2,3,4]]\n",
    "\n",
    "    z_stack_c = z_stack_c - z_stack_c.min(axis=(1,2,3), keepdims=True) # min-max scale across channels to maintain relative intensity\n",
    "    z_stack_c = z_stack_c / z_stack_c.max(axis=(1,2,3), keepdims=True)\n",
    "\n",
    "    z_stack_BF = z_stack_in.copy() # handle BF appart from fluorescence data as it's not fluorescence\n",
    "    z_stack_BF = z_stack_BF[:,:,:,5]\n",
    "    z_stack_BF = np.expand_dims(z_stack_BF, axis=-1)\n",
    "    z_stack_out = np.concatenate((z_stack_c,z_stack_BF), axis = -1)\n",
    "    \n",
    "    z_stack_UNET = z_stack_in[:,:,:,6] # append binary just in case\n",
    "    z_stack_UNET = np.expand_dims(z_stack_UNET, axis=-1)\n",
    "    z_stack_out = np.concatenate((z_stack_c,z_stack_UNET), axis = -1)\n",
    "    return(z_stack_out)\n",
    "\n",
    "def visualize_embeddings(embeddings, labels, method='umap', manual_colors = ['#FF0000', '#000000', '#990066', '#999999', '#006600', '#6600CC']):\n",
    "    labels = np.array(labels)\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    # Check if there are enough colors\n",
    "    if len(unique_labels) > len(manual_colors):\n",
    "        raise ValueError(f\"Not enough manual colors defined for {len(unique_labels)} labels.\")\n",
    "\n",
    "    # Create mapping using only as many colors as needed\n",
    "    label_to_color = {label: manual_colors[i] for i, label in enumerate(unique_labels)}\n",
    "\n",
    "    # Dimensionality reduction\n",
    "    if method == 'umap':\n",
    "        reducer = umap.UMAP(n_neighbors=15, min_dist=0.1, random_state=710)\n",
    "    elif method == 'tsne':\n",
    "        from sklearn.manifold import TSNE\n",
    "        reducer = TSNE(n_components=2, perplexity=30, learning_rate='auto', init='pca', random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"method must be 'umap' or 'tsne'\")\n",
    "\n",
    "    reduced = reducer.fit_transform(embeddings)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    for label in unique_labels:\n",
    "        idx = labels == label\n",
    "        plt.scatter(reduced[idx, 0], reduced[idx, 1], color=label_to_color[label],\n",
    "                    label=f'Class {label}', s=10, alpha=0.8)\n",
    "\n",
    "    plt.title(f\"Embedding Visualization ({method.upper()})\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend(markerscale=2, loc='best', fontsize=8)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return(reduced) # return the coordinates so we can inspect cells\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "autoencoder = load_model(autoencoder_path)\n",
    "\n",
    "encoder = Model(inputs=autoencoder.input,\n",
    "                outputs=autoencoder.layers[6].output)\n",
    "\n",
    "import joblib\n",
    "clf = joblib.load(classifier_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file names\n",
    "file_names_qa = ctc.parse_nd2_paths(img_dir_qa, \".cells.tiff\", recursive=False)\n",
    "file_names_qa = [s.replace(\".cells.tiff\", \"\") for s in file_names_qa]\n",
    "file_names_qa = file_names_qa\n",
    "print(len(file_names_qa)), file_names_qa[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = { # initialize data object\n",
    "\"sample_ID\": [],\n",
    "\"cells_raw\": [],\n",
    "\"embeddings\": [],\n",
    "\"labels_pred\": [],\n",
    "\"PofZ\": [],\n",
    "\"latent_features\": []\n",
    "}\n",
    "\n",
    "for r in range(len(file_names_qa)):\n",
    "        file_qa = file_names_qa[r]\n",
    "        print(\"Reading input...\")\n",
    "        print(file_qa)\n",
    "        \n",
    "        features_qa = pd.read_csv(img_dir_qa+file_qa+\".region_features.tsv\", sep = \"\\t\")\n",
    "        features_qa = features_qa[features_qa['class_cleanSeg'] == \"Cell\"]\n",
    "        features_qa['idx_0'] = range(len(features_qa))\n",
    "        features_cNames = features_qa.columns\n",
    "        target_cells_qa = features_qa\n",
    "        target_idx_qa = target_cells_qa['idx_0']\n",
    "\n",
    "        if target_cells_qa.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        ##### read image, normalize, predict\n",
    "        img_qa_raw = tifffile.imread(img_dir_qa+file_qa+\".cells.tiff\") # get stack of cells\n",
    "        if len(img_qa_raw.shape) == 3: # handle one cell inputs\n",
    "            img_qa_raw = img_qa_raw[np.newaxis,:]\n",
    "        img_qa_raw = img_qa_raw.transpose(0,2,3,1) # c last\n",
    "        print(\"########\")\n",
    "        print(len(features_qa))\n",
    "        print(img_qa_raw.shape)\n",
    "        \n",
    "        img_qa_raw = img_qa_raw[target_cells_qa['idx_0']]\n",
    "\n",
    "        c_names = ctc.get_channel_names_tiff(img_dir_qa+file_qa+\".cells.tiff\") # normalize CNN input, protein data is normalize apart from BF and UNET channels\n",
    "        print(c_names)\n",
    "        \n",
    "        c_exclusion = next((i for i, s in enumerate(c_names) if s.startswith(exclusion_name)), -1)\n",
    "        c_pCK = next((i for i, s in enumerate(c_names) if s.startswith(pCK_name)), -1)\n",
    "        channel_indices_pred = [c_exclusion,c_pCK]\n",
    "\n",
    "        if np.min(img_qa_raw[:,:,:,channel_indices_pred].max(axis=(1,2,3))) == 0:\n",
    "             # if a predictive channel was not recorded, the sample cannot be used and is skipped       \n",
    "             # the channel is, however, present as an array of 0's as a result of pre-processing\n",
    "             continue \n",
    "                      \n",
    "        img_qa_norm = ctc.normalize_CNN_input_chN(img_qa_raw, c_names)      \n",
    "        img_qa_CNN_in = img_qa_norm[:,:,:,channel_indices_pred]\n",
    "\n",
    "        print(\"embedding...\")\n",
    "        with tf.device(f'/gpu:{gpu_id}'):  # Use the specified GPU\n",
    "            embeddings_qa = encoder.predict(img_qa_CNN_in, verbose=0)\n",
    "\n",
    "        embeddings_qa = embeddings_qa.reshape(embeddings_qa.shape[0], -1) # Flatten embeddings\n",
    "\n",
    "        y_pred_qa = clf.predict(embeddings_qa) # predict label (0: CTC, 1: non-CTC)\n",
    "        y_pred_qa = np.expand_dims(y_pred_qa, axis=-1)\n",
    "        y_pred_prob_qa = clf.predict_proba(embeddings_qa)\n",
    "        y_pred_res_qa = np.concatenate([y_pred_qa,y_pred_prob_qa],axis = 1)\n",
    "\n",
    "        # encapsulate results\n",
    "        print(\"Input sample: \"+file_qa)\n",
    "        print(\"N input cells: \"+str(img_qa_raw.shape[0]))\n",
    "\n",
    "        res_dict[\"sample_ID\"].append([file_qa]*img_qa_raw.shape[0])\n",
    "        res_dict[\"cells_raw\"].append(img_qa_raw)\n",
    "        res_dict[\"embeddings\"].append(embeddings_qa)\n",
    "        res_dict[\"labels_pred\"].append(y_pred_qa)\n",
    "        res_dict[\"PofZ\"].append(y_pred_prob_qa)\n",
    "        res_dict[\"latent_features\"].append(embeddings_qa)\n",
    "        \n",
    "        print(\"#####\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### threshold for review purposes\n",
    "th_PofZ_0 = 0.75\n",
    "\n",
    "all_cells_raw_qa = res_dict[\"cells_raw\"]\n",
    "all_cells_raw_qa = np.concatenate(all_cells_raw_qa)\n",
    "all_y_pred_qa = res_dict[\"labels_pred\"].copy()\n",
    "all_y_pred_qa = np.concatenate(all_y_pred_qa)\n",
    "all_PofZ_qa = res_dict[\"PofZ\"].copy()\n",
    "all_PofZ_qa = np.concatenate(all_PofZ_qa)\n",
    "all_embeddings_qa = res_dict[\"embeddings\"].copy()\n",
    "all_embeddings_qa = np.concatenate(all_embeddings_qa)\n",
    "\n",
    "\n",
    "all_y_pred_qa[all_y_pred_qa==0]=10 # PofZ = 0.5\n",
    "all_y_pred_qa[all_y_pred_qa==1]=11\n",
    "count_labels(all_y_pred_qa)\n",
    "\n",
    "low_prob_0 = np.where(all_PofZ_qa[:,0] < th_PofZ_0) # PofZ = user definition\n",
    "all_y_pred_qa[low_prob_0]=11 \n",
    "count_labels(all_y_pred_qa)\n",
    "\n",
    "y_pred_qa_vis = y_pred_qa.copy() # format for visualization\n",
    "y_pred_qa_vis = y_pred_qa_vis.squeeze()\n",
    "all_y_pred_qa=all_y_pred_qa.squeeze()\n",
    "\n",
    "##### visualize to assess how distinct CTCs are at first pass\n",
    "idx_pred_11 = np.asarray(np.where(all_y_pred_qa == 11))\n",
    "idx_pred_11= idx_pred_11.squeeze()\n",
    "vis_11_embeddings_qa = all_embeddings_qa[idx_pred_11]\n",
    "vis_n_11 = np.min([2000,len(idx_pred_11)])\n",
    "\n",
    "idx_pred_11 = np.random.choice(range(idx_pred_11.shape[0]), size=vis_n_11, replace=False)\n",
    "vis_11_embeddings_qa = vis_11_embeddings_qa[idx_pred_11]\n",
    "vis_11_labs = [11]*vis_11_embeddings_qa.shape[0]\n",
    "\n",
    "idx_pred_10 = np.where(all_y_pred_qa == 10)\n",
    "vis_10_embeddings_qa = all_embeddings_qa[idx_pred_10]\n",
    "vis_10_labs = all_y_pred_qa[idx_pred_10]\n",
    "\n",
    "vis_n_10 = np.min([500,len(idx_pred_10[0])])\n",
    "idx_pred_10 = np.random.choice(range(idx_pred_10[0].shape[0]), size=vis_n_10, replace=False)\n",
    "vis_10_embeddings_qa = vis_10_embeddings_qa[idx_pred_10]\n",
    "vis_10_labs = [10]*vis_10_embeddings_qa.shape[0]\n",
    "\n",
    "embeddings_plot_qa = np.concatenate([vis_10_embeddings_qa,vis_11_embeddings_qa], axis = 0)\n",
    "labels_plot_qa = np.concatenate([vis_10_labs,vis_11_labs])\n",
    "umap_coords = visualize_embeddings(embeddings_plot_qa, labels_plot_qa, method='umap', manual_colors = ['#FF0000', '#000000', '#990066', '#CC0033', '#999999'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## RUN INFERENCE ON INDEPENDENT SAMPLES\n",
    "# PRINT OUT FEATURE TABLES WITH THE PREDICTIONS INCLUDED FOR ANALYSIS WITH CLINICAL DATA\n",
    "all_sample_ID_qa = res_dict[\"sample_ID\"]\n",
    "all_sample_ID_qa = np.concatenate(all_sample_ID_qa)\n",
    "all_sample_ID_qa_unique = list(set(all_sample_ID_qa))\n",
    "\n",
    "for sample_ID_qa in all_sample_ID_qa_unique:\n",
    "        cells_qa = all_cells_raw_qa[np.where(all_sample_ID_qa == sample_ID_qa)]\n",
    "        print(sample_ID_qa)\n",
    "\n",
    "        features_qa = pd.read_csv(img_dir_qa+sample_ID_qa+\".region_features.tsv\", sep = \"\\t\")\n",
    "        features_qa = features_qa[features_qa['class_cleanSeg'] == \"Cell\"]\n",
    "        features_qa['idx_0'] = range(len(features_qa))\n",
    "\n",
    "        review_idx_qa = np.asarray(np.where(all_sample_ID_qa == sample_ID_qa)).squeeze()\n",
    "        if review_idx_qa.shape == ():\n",
    "            continue\n",
    "        \n",
    "        print(\"N cells predicted: \"+str(len(review_idx_qa))+\" (at PofZ = \"+str(th_PofZ_0)+\")\")\n",
    "\n",
    "        features_cNames = features_qa.columns\n",
    "        target_cells_qa = features_qa\n",
    "\n",
    "        target_cells_qa['y_pred'] = all_y_pred_qa[review_idx_qa].ravel()\n",
    "        target_cells_qa['PofZ_0'] = all_PofZ_qa[review_idx_qa,0].ravel()\n",
    "        target_cells_qa['PofZ_1'] = all_PofZ_qa[review_idx_qa,1].ravel()\n",
    "        target_cells_qa['latent_features'] = list(all_embeddings_qa[review_idx_qa,:])#.ravel()\n",
    "        \n",
    "        target_cells_qa['idx_pred'] = np.arange(len(target_cells_qa))\n",
    "        print(\"N CTCs predicted: \"+str(len(target_cells_qa[target_cells_qa['y_pred']==10])))\n",
    "        cell_features_qa = target_cells_qa.copy()        \n",
    "\n",
    "        df_p = cell_features_qa\n",
    "        cell_features_qa['y_pred'] = cell_features_qa['y_pred'].fillna(9)\n",
    "\n",
    "        CTC_features_qa = cell_features_qa[cell_features_qa['y_pred']!=9] # print all examined cells so we can access thresholding\n",
    "        CTC_features_qa.to_csv(img_dir_qa+sample_ID_qa+\".cell_features.csv\")\n",
    "        print(\"---\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2D_CTC_pr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
