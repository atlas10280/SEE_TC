{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "########## Dependencies\n",
    "##### define neural net weights\n",
    "model_path = \"/mnt/local/data2/Bootsma/2D_CTC/src/analysis/publication_code/weights/01_SEE_TC_classify_single_cells.hdf5\"\n",
    "\n",
    "##### define samples to process\n",
    "img_dir = \"/mnt/local/data2/Bootsma/2D_CTC/src/analysis/publication_code/test_data/\"\n",
    "\n",
    "##### define GPU indices to use\n",
    "gpu_index = \"0\"  # Specify the GPUs you want to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "##### define suffix for input tiff\n",
    "input_tiff_suffix = \".bias_corrected.tiff\"\n",
    "\n",
    "##### define suffix for output table \n",
    "# must match the suffix of your physical feature table\n",
    "# results will be appended into said table\n",
    "feature_suffix = \".region_features.tsv\"\n",
    "\n",
    "##### Libraries\n",
    "import sys\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from scipy.ndimage import label\n",
    "from skimage.measure import regionprops\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "sys.path.append('../src/') \n",
    "import SEE_TC as ctc\n",
    "##### load model\n",
    "with tf.device(f'/GPU:{gpu_index}'):\n",
    "    classifier = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input files\n",
    "file_names = ctc.parse_nd2_paths(img_dir, input_tiff_suffix, recursive=True)\n",
    "sample_IDs = [s.replace(input_tiff_suffix, \"\") for s in file_names]\n",
    "print(len(sample_IDs))\n",
    "print(sample_IDs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_values = ['Cell', 'Doubleton', 'Cluster', 'Debris']\n",
    "probsCS_names = ['probCS_cell', 'probCS_doubleton', 'probCS_cluster', 'probCS_debris']\n",
    "\n",
    "\n",
    "for i in range(len(sample_IDs)):\n",
    "        sample_ID_i = sample_IDs[i]\n",
    "\n",
    "        print(\"Processing \"+sample_ID_i)\n",
    "        # load image\n",
    "        img_i = tifffile.imread(sample_ID_i+input_tiff_suffix) # get image\n",
    "        c_names = ctc.get_channel_names_tiff(sample_ID_i+input_tiff_suffix)\n",
    "\n",
    "        # use channel name recognition to extract predictive channels\n",
    "        c3 = [i for i, s in enumerate(c_names) if s.startswith('3')][0]\n",
    "        c_UNET = [i for i, s in enumerate(c_names) if s.startswith('UNET')][0]\n",
    "        c_to_use = [c_UNET,c3] # binary and Hoechst\n",
    "\n",
    "        img_i = img_i.transpose(1,2,0) # channel last\n",
    "        binary_i = img_i[:,:,c_UNET]\n",
    "        ######### Handle 20x\n",
    "        input_magnification = ctc.get_magnification(sample_ID_i+\".nd2\")\n",
    "                \n",
    "        if input_magnification != 10:\n",
    "                scale_factor = input_magnification/10 # unet is trained at 10x so scale to fit that\n",
    "                img_i = ctc.downscale_by_factor_of(img_i, scale_factor) # channel last\n",
    "                binary_i = ctc.downscale_by_factor_of(binary_i, scale_factor) # channel last\n",
    "\n",
    "        #########\n",
    "\n",
    "        # Assuming image_array and binary_array are your inputs\n",
    "        image_array = img_i\n",
    "        binary_array = binary_i\n",
    "        binary_array[binary_array>0]=255 # CNN was trained on 0-255 for binary... \n",
    "\n",
    "        labeled_array, n_obj = label(binary_array) # Label the binary regions\n",
    "        regions = regionprops(labeled_array) # Extract region properties\n",
    "\n",
    "        tile_size = 32 # Define the size of the tile\n",
    "        half_tile_size = tile_size // 2\n",
    "\n",
    "        # Function to extract a tile for a given region\n",
    "        def extract_tile(region):\n",
    "                centroid = region.centroid\n",
    "                x, y = int(centroid[1]), int(centroid[0])\n",
    "\n",
    "                # Calculate start and end indices for slicing\n",
    "                start_x = x - half_tile_size\n",
    "                end_x = x + half_tile_size\n",
    "                start_y = y - half_tile_size\n",
    "                end_y = y + half_tile_size\n",
    "                \n",
    "                # Initialize a zero-padded tile\n",
    "                tile = np.zeros((tile_size, tile_size, image_array.shape[2]))\n",
    "                # tile_bin = np.zeros((tile_size, tile_size))\n",
    "\n",
    "                # Calculate valid ranges within the image\n",
    "                img_start_x = max(0, start_x)\n",
    "                img_end_x = min(image_array.shape[1], end_x)\n",
    "                img_start_y = max(0, start_y)\n",
    "                img_end_y = min(image_array.shape[0], end_y)\n",
    "\n",
    "                # Calculate valid ranges within the tile\n",
    "                tile_start_x = max(0, -start_x)\n",
    "                tile_end_x = tile_size - max(0, end_x - image_array.shape[1])\n",
    "                tile_start_y = max(0, -start_y)\n",
    "                tile_end_y = tile_size - max(0, end_y - image_array.shape[0])\n",
    "                \n",
    "                # Copy the valid region from the image to the tile\n",
    "                tile[tile_start_y:tile_end_y, tile_start_x:tile_end_x, :] = image_array[img_start_y:img_end_y, img_start_x:img_end_x, :]\n",
    "                # tile_bin[tile_start_y:tile_end_y, tile_start_x:tile_end_x] = binary_array[img_start_y:img_end_y, img_start_x:img_end_x]\n",
    "                # print(tile.shape, tile_bin.shape)\n",
    "                return tile\n",
    "        def extract_tile_bin(region):\n",
    "                centroid = region.centroid\n",
    "                x, y = int(centroid[1]), int(centroid[0])\n",
    "\n",
    "                # Calculate start and end indices for slicing\n",
    "                start_x = x - half_tile_size\n",
    "                end_x = x + half_tile_size\n",
    "                start_y = y - half_tile_size\n",
    "                end_y = y + half_tile_size\n",
    "                \n",
    "                # Initialize a zero-padded tile\n",
    "                # tile = np.zeros((tile_size, tile_size, image_array.shape[2]))\n",
    "                tile_bin = np.zeros((tile_size, tile_size))\n",
    "\n",
    "                # Calculate valid ranges within the image\n",
    "                img_start_x = max(0, start_x)\n",
    "                img_end_x = min(image_array.shape[1], end_x)\n",
    "                img_start_y = max(0, start_y)\n",
    "                img_end_y = min(image_array.shape[0], end_y)\n",
    "\n",
    "                # Calculate valid ranges within the tile\n",
    "                tile_start_x = max(0, -start_x)\n",
    "                tile_end_x = tile_size - max(0, end_x - image_array.shape[1])\n",
    "                tile_start_y = max(0, -start_y)\n",
    "                tile_end_y = tile_size - max(0, end_y - image_array.shape[0])\n",
    "                \n",
    "                # Copy the valid region from the image to the tile\n",
    "                # tile[tile_start_y:tile_end_y, tile_start_x:tile_end_x, :] = image_array[img_start_y:img_end_y, img_start_x:img_end_x, :]\n",
    "                tile_bin[tile_start_y:tile_end_y, tile_start_x:tile_end_x] = binary_array[img_start_y:img_end_y, img_start_x:img_end_x]\n",
    "                # print(tile.shape, tile_bin.shape)\n",
    "                return tile_bin\n",
    "\n",
    "        print(\"Normalizing stack for CNN...\")\n",
    "        # Parallel extraction of tiles\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "                tiles = list(executor.map(extract_tile, regions))\n",
    "        tiles = np.stack(tiles, axis = 0)\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "                tiles_bins = list(executor.map(extract_tile_bin, regions))\n",
    "        tiles_bins = np.stack(tiles_bins, axis = 0)\n",
    "\n",
    "        # normalize at the group level, i.e., across tiles. Accounting for outliers (clipping above/below 1.5*IQR)\n",
    "        tiles_norm = ctc.preprocess_autoEncoder_01(tiles)\n",
    "        tiles_hoechst = tiles_norm[:,:,:,c3]\n",
    "        \n",
    "        tiles_bins[tiles_bins > 0] = 255\n",
    "        tiles_bins = np.expand_dims(tiles_bins, axis=-1)  \n",
    "        z_stack = np.concatenate((tiles_bins, np.expand_dims(tiles_hoechst,-1)), axis = 3) # merge all channels with binary\n",
    "\n",
    "        print(\"Stack size:\")\n",
    "        print(z_stack.shape)\n",
    "\n",
    "        input_dict = {} # initialize dictionary\n",
    "        input_dict[sample_ID_i] = z_stack\n",
    "        ############## APPLY CLEAN SEG ############\n",
    "        input_dict = dict(list(input_dict.items()))        \n",
    "        \n",
    "        output_dict = {}\n",
    "        for key, array in input_dict.items():\n",
    "                \n",
    "                input_for_model = array[:, :, :, :]  # shape (n, 32, 32, 2)\n",
    "                \n",
    "                # Pass through the classification model\n",
    "                with tf.device('CPU'):\n",
    "                        img_tensor = tf.convert_to_tensor(input_for_model)\n",
    "                with tf.device(f'/GPU:{gpu_index}'):\n",
    "                        class_probabilities = classifier.predict(img_tensor)\n",
    "                # Determine the class with the highest probability\n",
    "                class_indices = np.argmax(class_probabilities, axis=1)\n",
    "                class_assignments = np.array(class_values)[class_indices]\n",
    "\n",
    "                # Create tuples with the original array, assigned class, and class probabilities\n",
    "                output_dict[key] = (array, class_assignments, class_probabilities)\n",
    "        \n",
    "        #########################\n",
    "        print(\"Appending results to features...\")\n",
    "        cleanSeg_i = output_dict[sample_ID_i]\n",
    "        classes_i = cleanSeg_i[1]\n",
    "        indices_of_cells = [index for index, item in enumerate(classes_i) if item == \"Cell\"]\n",
    "        print(\"Cells detected: \"+str(len(indices_of_cells)))\n",
    "        \n",
    "        features_i = pd.read_csv(sample_ID_i+feature_suffix, sep='\\t')\n",
    "        features_i['class_cleanSeg'] = classes_i# append cleanSeg classes to the feature table\n",
    "        \n",
    "        probsCS = cleanSeg_i[2] # extract cleanSeg probabilities \n",
    "        probsCS_df = pd.DataFrame(probsCS, columns=probsCS_names)\n",
    "        # append cleanSeg probabilities to the feature table, remove common columns if they exist ()\n",
    "        common_columns = features_i.columns.intersection(probsCS_df.columns)\n",
    "        features_i = features_i.drop(columns=common_columns)\n",
    "        features_i = pd.concat([features_i, probsCS_df], axis=1)\n",
    "        features_i.to_csv(sample_ID_i+feature_suffix, sep = \"\\t\", index = False) # write output\n",
    "\n",
    "        ###### export cell stack for review and class-specific parsing\n",
    "        print(\"writing tiles...\")\n",
    "        tiles_out = np.transpose(tiles, (0,3,1,2))\n",
    "        print(tiles_out.shape)\n",
    "        tiles_out = tiles_out[indices_of_cells,:,:,:]\n",
    "        ############ fill missing channels with 0's\n",
    "        if len(c_names) < 7:\n",
    "                print(\"APPENDING MISSING CHANNELS AS ZERO...\")\n",
    "                tiles_out = np.transpose(tiles_out, (0,2,3,1))\n",
    "                print(tiles_out.shape)\n",
    "                tiles_out = ctc.append_missing_channels(tiles_out, c_names) # append 0's after normalization so as to not introduce errors there. Need them present so we index predictive channel correctly (hard coded)\n",
    "                c_names_write = ['300', '400', '500', '600', '700', 'BF', 'UNET_open']\n",
    "                tiles_out = np.transpose(tiles_out, (0,3,1,2))\n",
    "                print(tiles_out.shape)\n",
    "        else:\n",
    "                c_names_write = c_names.copy()\n",
    "        ############\n",
    "        tiles_out = tiles_out.astype(np.uint16)\n",
    "        tifffile.imwrite(sample_ID_i+\".cells.tiff\",\n",
    "                                data=tiles_out,\n",
    "                                metadata={\n",
    "                                        'axes':'ZCYX',\n",
    "                                        'spacing':0.65,\n",
    "                                        'orientation':'topleft',\n",
    "                                        \"Channel\":{\"Name\":c_names_write}\n",
    "                                        },\n",
    "                        ome=True\n",
    "                        )\n",
    "\n",
    "        print(\"##### Done #####\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2D_CTC_pr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
