{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Dependencies\n",
    "##### define UNET weights\n",
    "model_path = \"/mnt/local/data2/Bootsma/2D_CTC/src/analysis/publication_code/weights/00_SEE_TC_UNET.hdf5\"\n",
    "\n",
    "##### define samples to process\n",
    "img_dir = \"/mnt/local/data2/Bootsma/2D_CTC/src/analysis/publication_code/test_data/\"\n",
    "\n",
    "##### define GPU indices to use\n",
    "GPU_2_use = 1 ##### define GPU indices to use\n",
    "GPU_ids = [str(GPU_2_use)]  # Specify the GPUs you want to use\n",
    "\n",
    "##### Dependencies\n",
    "import os\n",
    "import sys\n",
    "import nd2\n",
    "import tifffile\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from scipy import ndimage\n",
    "from keras.models import load_model\n",
    "\n",
    "sys.path.append('../src/') \n",
    "import SEE_TC as ctc\n",
    "\n",
    "##### load model\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(GPU_2_use)\n",
    "with tf.device(f'/gpu:{str(GPU_2_use)}'):  # Use the specified GPU\n",
    "    model = load_model(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse inputs (expects .nd2 images)\n",
    "files = ctc.parse_nd2_paths(img_dir, \"nd2\", recursive=True)\n",
    "files = [s.replace(\".nd2\", \"\") for s in files]\n",
    "print(len(files), files[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(files)):\n",
    "    print(\"-------------\"+str(i)+\"------------\")\n",
    "    file_i = files[i]\n",
    "    img_path_i = file_i+\".nd2\"\n",
    "\n",
    "    ########### check the magnificaiton\n",
    "    with nd2.ND2File(img_path_i) as ndfile:\n",
    "        text_unparsed = ndfile.text_info\n",
    "        cap_settings = text_unparsed['capturing']\n",
    "        optics = text_unparsed['optics']\n",
    "    if '10x' in optics:\n",
    "        input_magnification = 10\n",
    "        \n",
    "        if os.path.isfile(img_path_i) == False:\n",
    "            print(\"skipping sample (\"+file_i+\") no input\")\n",
    "            print(\"###############\\n\")            \n",
    "            continue\n",
    "        \n",
    "    elif '20x' in optics:\n",
    "            input_magnification = 20\n",
    "    elif '40x' in optics:\n",
    "            input_magnification = 40\n",
    "    else:\n",
    "        print(\"skipping sample (\"+file_i+\"), unknown magnification\")\n",
    "        print(\"###############\\n\")            \n",
    "        continue\n",
    "    \n",
    "    ############# check if already processed\n",
    "    if os.path.isfile(file_i+\".bin_open.tiff\"):\n",
    "            print(\"skipping sample (\"+file_i+\"), already processed\")\n",
    "            print(\"###############\\n\")            \n",
    "            continue\n",
    "    \n",
    "    ############# READ RAW ARRAY #############\n",
    "    img_i, c_names = ctc.extract_all_array_nd2(img_path_i)\n",
    "    img_i = np.transpose(img_i, (1,2,0)) # channel last\n",
    "    c = next((i for i, s in enumerate(c_names) if \"350\" in s or \"390\" in s), -1)\n",
    "    \n",
    "    print(file_i)\n",
    "    print(img_i.shape, c_names, c)\n",
    "    \n",
    "    ##### Segment #####   \n",
    "    # 10x\n",
    "    if input_magnification == 10:\n",
    "        bin_i = ctc.segment_image(img_i, c, model, window_size=(32, 32), step_size=16, mode = \"GPU\", GPU_ids = GPU_ids)            \n",
    "    # non-10x\n",
    "    else:\n",
    "        scale_factor = input_magnification/10 # unet is trained at 10x so scale to fit that\n",
    "        print(\"re-scaling input by factor of \"+str(scale_factor))\n",
    "        img_i = ctc.downscale_by_factor_of(img_i, scale_factor)\n",
    "        print(img_i.shape)\n",
    "        bin_i = ctc.segment_image(img_i, c, model, window_size=(32, 32), step_size=16, mode = \"GPU\", GPU_ids = GPU_ids)\n",
    "        print('scaling back to input size...')\n",
    "        bin_i = ctc.downscale_by_factor_of(bin_i, 1/scale_factor) # scale binary up to input size            \n",
    "    \n",
    "    print(\"writing binary segmentation array...\")\n",
    "    structuring_element = np.ones((3, 3))\n",
    "    opened_image = ndimage.binary_opening(bin_i, structure=structuring_element, iterations=2)\n",
    "    tifffile.imwrite(file_i+\".bin_open.tiff\", data=opened_image.astype(np.uint16))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2D_CTC_pr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
