{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Thu Jun 15 07:15:42 2023\n",
    "\n",
    "@author: mbootsma\n",
    "\n",
    "using 3 channel (hoechst, inclusion, exclusion) input with float32\n",
    "normalization is log-transform, z-scale, min-max\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "sys.path.append('../src/') \n",
    "import SEE_TC as ctc\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nd2\n",
    "from skimage import restoration\n",
    "from sklearn.model_selection import KFold\n",
    "import tensorflow as tf\n",
    "\n",
    "# USER VARS\n",
    "BIN_ID = \"cells\" #define the binary layer name to extract\n",
    "CHANNEL_1 = \"350\" #Hoechst\n",
    "N_EPOCH = 1000#define the number of epochs to train for\n",
    "\n",
    "path = \"/mnt/Data02/Bootsma/project_CTC_ML/CTC_ML/\"\n",
    "GPU_ids = [\"3\"]  # Specify the GPUs you want to use\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "gpu_index = '3'  # Change this to the desired GPU index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_CHANNELS = 3 #\n",
    "SIZE_BATCH = 16 # batch size for gpu\n",
    "SIZE_IMG = 32 # tile size to yield\n",
    "BACKGROUND_PRP = 0.5 # rate at which to thin background\n",
    "c=0 # Hoechst is on channel 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path = \"../train_data/01_train_UNET_slide_00.nd2\"\n",
    "img_HD_path = \"../train_data/01_train_UNET_slide_01.nd2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column 'value_ID' and populate it with elements\n",
    "df_results = pd.DataFrame({'rownames' : ['val_loss', 'val_accuracy']})\n",
    "\n",
    "\n",
    "with nd2.ND2File(img_path) as metadata:\n",
    "    ################### read and pre-process\n",
    "    img = ctc.extract_image_3Channel(img_path, CHANNEL_1,CHANNEL_1,CHANNEL_1)\n",
    "    img = img.transpose(1,2,0) # place channel last\n",
    "\n",
    "    img_i = img.astype(np.float32) # convert for processing\n",
    "    # Clip each channel prior to FF estimation\n",
    "    print(\"Clipping...\") # clip outliers\n",
    "    \n",
    "    image_clipped = ctc.adaptive_clipping_with_iqr(img_i[..., c]) \n",
    "    \n",
    "    print(\"Flattening...\") # Estimate flat field for each clipped channel\n",
    "    flat_field_c = ctc.estimate_flat_field(image_clipped)\n",
    "    flat_field_c = flat_field_c/np.max(flat_field_c) # normalize the correction mask\n",
    "\n",
    "    image_flat = img_i[..., c] / flat_field_c # APPLY FLAT FIELD CORRECTION HERE\n",
    "\n",
    "    # Normalize corrected image to the original dtype range (e.g., 0-65535 for uint16)\n",
    "    image_flat = (image_flat * 65535 / np.max(image_flat)).astype(np.uint16)\n",
    "\n",
    "    print(\"Subtracting background...\") # Subtract background\n",
    "    x = image_flat\n",
    "    background_x = restoration.rolling_ball(x, radius = 21) # radius should be ~2x object size...\n",
    "    x_rb = x-background_x\n",
    "\n",
    "    img_2_seg = np.stack((x_rb,x_rb,x_rb), axis = -1) # extract single sample\n",
    "\n",
    "    img_2_seg[np.where(img_2_seg == 65535)] = 65534\n",
    "    img_2_seg = img_2_seg+1\n",
    "    img = np.log10(img_2_seg)                   \n",
    "    mean = np.mean(img_2_seg)\n",
    "    std_dev = np.std(np.float64(img_2_seg), dtype=np.float64) # run as float64 to prevent overflow\n",
    "    std_dev = np.float16(std_dev) # convert back to 16 for calculation\n",
    "    img_2_seg = (img_2_seg-mean)/std_dev\n",
    "\n",
    "    ##################\n",
    "    img_patches = ctc.patchify_image_3Channel(img_2_seg, SIZE_IMG)# make patches of each layer                            \n",
    "    binary = ctc.extract_binary(img_path, BIN_ID)# load binary layer                           \n",
    "    binary_patches = ctc.patchify_image(binary, SIZE_IMG)        \n",
    "    \n",
    "    img_patches, binary_patches = ctc.thin_background_rankedBackground(img_patches, binary_patches, BACKGROUND_PRP) #reduce the proportion of images with no cells present\n",
    "    training_img = img_patches\n",
    "    training_binary = binary_patches\n",
    "            \n",
    "######################## TRAIN MODEL ######################\n",
    "# split data into train and test (last 10% of image)\n",
    "n_tiles = training_img.shape[0]\n",
    "split_idx = int(n_tiles*0.9)\n",
    "\n",
    "test_img = training_img[split_idx+1:n_tiles,:,:,:]\n",
    "test_binary = training_binary[split_idx+1:n_tiles,:,:,:]\n",
    "\n",
    "training_img = training_img[0:split_idx,:,:,:]\n",
    "training_binary = training_binary[0:split_idx,:,:,:]\n",
    "\n",
    "# define fold indices for k-fold training\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "# train model\n",
    "print(\"input shape (img/bin training/validation):\")\n",
    "print(training_img.shape, training_binary.shape, test_img.shape, test_binary.shape)\n",
    "\n",
    "for fold, (train_index, val_index) in enumerate(kf.split(training_img, training_binary)):\n",
    "\n",
    "    print(f\"Fold {fold + 1}/{n_splits}\")                \n",
    "    X_train, X_val = training_img[train_index], training_img[val_index]\n",
    "    y_train, y_val = training_binary[train_index], training_binary[val_index]\n",
    "\n",
    "    ###################### Load High Density samples after defining folds so we always have 50-50 split on them\n",
    "    print(\"inserting high density (HD) samples\")\n",
    "\n",
    "    img_HD = ctc.extract_image_3Channel(img_HD_path, CHANNEL_1,CHANNEL_1,CHANNEL_1)\n",
    "    img_HD = img_HD.transpose(1,2,0) # place channel last\n",
    "\n",
    "    ######################\n",
    "    img_i = img_HD.astype(np.float32) # convert for processing\n",
    "\n",
    "    print(\"Clipping...\") # clip outliers\n",
    "    c=0\n",
    "    image_clipped = ctc.adaptive_clipping_with_iqr(img_i[..., c])     \n",
    "    print(\"Flattening...\") # Estimate flat field for each clipped channel\n",
    "    flat_field_c = ctc.estimate_flat_field(image_clipped)\n",
    "    flat_field_c = flat_field_c/np.max(flat_field_c) # normalize the correction mask\n",
    "    image_flat = img_i[..., c] / flat_field_c # APPLY FLAT FIELD CORRECTION HERE\n",
    "    image_flat = (image_flat * 65535 / np.max(image_flat)).astype(np.uint16)\n",
    "\n",
    "    print(\"Subtracting background...\") # Subtract background\n",
    "    x = image_flat\n",
    "    background_x = restoration.rolling_ball(x, radius = 21) # radius should be ~2x object size...\n",
    "    x_rb = x-background_x\n",
    "\n",
    "    img_HD = np.stack((x_rb,x_rb,x_rb), axis = -1) # extract single sample\n",
    "\n",
    "    img_HD[np.where(img_HD == 65535)] = 65534\n",
    "    img_HD = img_HD+1\n",
    "    img = np.log10(img_HD)                   \n",
    "    mean = np.mean(img_HD)\n",
    "    std_dev = np.std(np.float64(img_HD), dtype=np.float64) # run as float64 to prevent overflow\n",
    "    std_dev = np.float16(std_dev) # convert back to 16 for calculation\n",
    "    img_HD = (img_HD-mean)/std_dev\n",
    "    #############\n",
    "    img_HD_patches = ctc.patchify_image_3Channel(img_HD, SIZE_IMG)# make patches of each layer\n",
    "    binary_HD = ctc.extract_binary(img_HD_path, BIN_ID)# load binary layer                   \n",
    "    binary_HD_patches = ctc.patchify_image(binary_HD, SIZE_IMG)  \n",
    "    img_HD_patches, binary_HD_patches = ctc.thin_background_rankedBackground(img_HD_patches, binary_HD_patches, BACKGROUND_PRP) #reduce the proportion of images with no cells present\n",
    "    print(\"HD patch count: \")\n",
    "    print(img_HD_patches.shape)\n",
    "    # split high denisty data in half. use for train/validation\n",
    "    train_img_HD, train_binary_HD, test_img_HD, test_binary_HD = ctc.split_array_trainTest(img_HD_patches, binary_HD_patches, 0.5) \n",
    "\n",
    "    # insert the high density training data evenly into each fold\n",
    "    X_train = np.concatenate((X_train, train_img_HD), axis=0)\n",
    "    y_train = np.concatenate((y_train, train_binary_HD), axis=0)\n",
    "\n",
    "    X_val = np.concatenate((X_val, test_img_HD), axis=0)\n",
    "    y_val = np.concatenate((y_val, test_binary_HD), axis=0)\n",
    "    print(\"input shape (img/bin training/validation):\")\n",
    "    print(X_train.shape, y_train.shape, X_val.shape, y_val.shape)\n",
    "    ######################\n",
    "    with tf.device(f'/GPU:{gpu_index}'):\n",
    "        model = ctc.unet_small_objects_deep(SIZE_IMG, SIZE_IMG, N_CHANNELS)  # Create a new model for each fold\n",
    "        callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=200, mode = 'min', restore_best_weights= True)]\n",
    "\n",
    "        seed = 42\n",
    "        np.random.seed = seed\n",
    "\n",
    "        model.fit(X_train, y_train, \n",
    "                            validation_data = (X_val, y_val), \n",
    "                            batch_size = SIZE_BATCH, epochs = N_EPOCH, verbose = 1, shuffle = True, callbacks = callbacks)    \n",
    "        \n",
    "        # Evaluate the model on the current fold's validation data\n",
    "        scores = model.evaluate(test_img, test_binary, verbose=0)\n",
    "    print(f\"Validation loss: {scores[0]}, Validation accuracy: {scores[1]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "2D_CTC_pr1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
